
We use tree based algorithms and compare trees, bagging and random forest using the Golub dataset and analyze its results

```{r}
library(hdrm)
downloadData(Golub1999)
attachData(Golub1999)

set.seed(2)

n = nrow(X)
# Split data into train and test sets
train_rows <- sample(1:n, n/2)
X.train <- X[train_rows, ]
X.test <- X[-train_rows, ]

y.train <- y[train_rows]
y.test <- y[-train_rows]

dim(X.train)
```


```{r}
library(tree)
library(ISLR)
library(randomForest)
```

Now that we've imported the required libraries, let's go ahead and do the test and train dataset split. We can also convert the vectors into dataframes.

```{r}
set.seed(2)
main.df = data.frame(X,y)
main.df$y <- as.factor(main.df$y)
train.df <- main.df[train_rows, ]
test.df <- main.df[-train_rows, ]
```


We now train the tree with the train dataset. There aren't many branches  due to sample selection and less number of observations

```{r}
tree.mod = tree(train.df$y~., train.df)

plot(tree.mod)
text(tree.mod, pretty = 0)
tree.mod
```

```{r}
tree.pred = predict(tree.mod, test.df, type="class")

confusion_mat <- table(test.df$y, tree.pred)
confusion_mat
```

Based on the above confusion matrix the accuracy will be

```{r}
accuracy <- sum(diag(confusion_mat))/sum(confusion_mat)
accuracy
```

Next, we consider whether pruning the tree might lead to improved results. The function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration. We use the argument FUN = prune.misclass in order to indicate that we want the classification error rate as our cost function to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance. The cv.tree() function reports the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used

```{r}
prune.mod = cv.tree(tree.mod, FUN=prune.misclass)
prune.mod

```
```{r}
plot(prune.mod)
```


From the above plot we can identify that we have to set the parameter best = 2

```{r}
prune_r.mod = prune.misclass(tree.mod, best=2)
plot(prune_r.mod)
text(prune_r.mod, pretty=0)
```

```{r}
prune.pred = predict(prune_r.mod, test.df, type="class")
with(test.df, table(prune.pred, train.df$y))
```

```{r}
confusion_prune <- table(test.df$y, prune.pred)
confusion_prune
accuracy <- sum(diag(confusion_prune)) / sum(confusion_prune)
accuracy
```
This validates our earlier hypothesis that this would not lead to an improvement due to the nature of the data. The accuracy does not improve at all. It remains the same at 0.8611

Ensemble learning is an ML technique which combines several learning methods in order to produce one optimal model. In bagging each model is independently constructed using using a bootstrap sample of the dataset. Bagging helps 
reduce variance and by extension prevent overfitting. We can take the value of mtry as 7129 as it is the number of predictors for this dataset. 
Let the number of trees be 500 by default,

```{r}

set.seed(2)
bagging.mod <- randomForest(train.df$y~., data=train.df, mtry = 7129, importance = TRUE)

bagging.mod
```

```{r}
bagging.pred = predict(bagging.mod, newdata=test.df)

confusion_bagging <- table(test.df$y, bagging.pred)
confusion_bagging
```

Now let's calculate the accuracy

```{r}
bagging_acc <- sum(diag(confusion_bagging)) / sum(confusion_bagging)
bagging_acc
```

```{r}
plot(bagging.pred, test.df$y)
abline(0,1)
```
Whenever a split is considered in random forest, a random sample of m predictors are chosen. This is the reason behind the naming of random forest. We chose the mtry value as the square root of p. Hence mtry = 84. We also use tuneRF to determine the best mtry.

It uses OOB error estimate to find the optimal mtry.

```{r}
set.seed(2)
res <- tuneRF(train.df, train.df$y, stepFactor = 1.5)
res
```

In this case we observe the optimal m_try at 84 

```{r}
set.seed(2)
rf.mod = randomForest(train.df$y~., data=train.df, mtry=84, importance=TRUE)

rf.mod
```

```{r}
rf.pred = predict(rf.mod, newdata=test.df, type="class")

confusion_mat <- table(rf.pred, test.df$y)
confusion_mat
```

Calculating accuracy

```{r}
accuracy <- sum(diag(confusion_mat)) / sum(confusion_mat)
accuracy
```

Now to compare bagging with random forest over a range of ntrees. Just like previously mtry would be equal to number of predictors to perform bagging and 126 for random forest.

```{r}

ntreeset = seq(10, 610, by=200)
acc.bag = rep(0,length(ntreeset));acc.rf = rep(0,length(ntreeset))

for(i in 1:length(ntreeset)){
  nt = ntreeset[i]
  bag.mod <- randomForest(train.df$y~., data=train.df, mtry=7129, ntree=nt, importance=TRUE)
  bag.pred = predict(bag.mod, newdata=test.df)
  confusion_bag <- table(test.df$y, bag.pred)
  acc.bag[i] <- sum(diag(confusion_bag)) / sum(confusion_bag)
  rf.mod = randomForest(train.df$y~., data=train.df, mtry=126, ntree = nt, importance=TRUE)
  rf.pred = predict(rf.mod, newdata=test.df, type="class")
  confusion_rf <- table(test.df$y, rf.pred)
  acc.rf[i] <- sum(diag(confusion_rf)) / sum(confusion_rf)
}


```

```{r}
plot(ntreeset, acc.bag, type="l", col=2, ylim=c(0,1))
lines(ntreeset, acc.rf, col=3)
legend("bottomright", c("Bagging", "RF"), col=c(2,3), lty=c(1,1))
```

We plot the accuracy obtained at each iteration. From the plot we can see that bagging starts low and achieves a steady accuracy of 90%. Whereas RF is not worse but is still lesser than the bagging. Overall Bagging performs better than Random Forest in this case.

Another iteration of bagging with random forest over a range of ntrees and a different mtry value, mtry is n=number of predictors to perform bagging, 84(square root), 189(second optimum), and 126(optimized by tuneRF) for random forest.

```{r}
ntreeset = seq(10,610,by=200)
acc.bag = rep(0, length(ntreeset))
acc.rf1 = rep(0, length(ntreeset))
acc.rf2 = rep(0, length(ntreeset))
acc.rf3 = rep(0, length(ntreeset))

for(i in 1:length(ntreeset)){
  
  nt = ntreeset[i]
  
  print("Start")
  bag.mod = randomForest(train.df$y~., data=train.df,mtry=7129,ntree=nt)
  bag.pred = predict(bag.mod, newdata=test.df)
  confusion_bag <- table(test.df$y, bag.pred)
  acc.bag[i] <- sum(diag(confusion_bag)) / sum(confusion_bag)
  print("After bag")
  
  rf1.mod = randomForest(train.df$y~., data=train.df, mtry=84, ntree=nt)
  rf1.pred = predict(rf1.mod, newdata=test.df)
  confusion_rf1 <- table(test.df$y, rf1.pred)
  acc.rf1[i] <- sum(diag(confusion_rf1)) / sum(confusion_rf1)
  print("After rf1")
  
  rf2.mod = randomForest(train.df$y~., data=train.df, mtry=126, ntree=nt)
  rf2.pred = predict(rf2.mod, newdata=test.df)
  confusion_rf2 <- table(test.df$y, rf2.pred)
  acc.rf2[i] <- sum(diag(confusion_rf2)) / sum(confusion_rf2)
  print("After rf2")
  
  rf3.mod = randomForest(train.df$y~., data=train.df, mtry=84, ntree=nt)
  rf3.pred = predict(rf3.mod, newdata=test.df)
  confusion_rf3 <- table(test.df$y, rf3.pred)
  acc.rf3[i] <- sum(diag(confusion_rf3)) / sum(confusion_rf3)
  print("After rf3")
  
}
```

```{r}
acc.bag
acc.rf1
acc.rf2
acc.rf3
ntreeset
```


Let's plot the values

```{r}
plot(ntreeset, acc.bag, type="l", col=2, ylim=c(0,1))
lines(ntreeset, acc.rf1, col=3)
lines(ntreeset, acc.rf2, col=4)
lines(ntreeset, acc.rf3, col=5)

legend("bottomright", c("Bagging", "RF (m=84)", "RF (m=126)", "RF (m=189)"), col=c(2,3,4,5), lty=c(1,1,1,1))
```
To conclude, decision trees work well for the classification of high dimensional data but is not a good predictor due to increased variance. Decision trees are actually faster than bagging or random forest. They can be used for EDA but not so ideal for prediction.

Bagging is a bit more complicated due to the high volume of 'p' variables. It had the best accuracy out of the four as per the above plot.

Random Forest is not necessarily better when the mtry value is the square root of n. Using a tuning function tat reduced the OBB to find the optimal works better. Bagging had a better result than RF.

The trees in RF are more independent of each other than bagging, making it suitable for predictive analysis. The dataset however has high number of variables compared to the number of observations which leads to a high variance problem. The reason is because bagging is used as a solution to high variance problem.
